{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "---\n* *** \uc6d0\ubcf8 \uc18c\uc2a4\ub294 \ud64d\ucf69\uacfc\uae30\ub300 \uae40\uc131\ud6c8 \uad50\uc218\ub2d8\uc758  [Github link](https://github.com/hunkim/DeepLearningZeroToAll)\ub97c \ucc38\uc870\ud558\uc138\uc694 ***     \n* *** Watson studio notebook \uc5d0\uc11c \uc791\uc5c5 \uac00\ub2a5\ud558\ub3c4\ub85d \uc218\uc815\ud558\uc600\uc73c\uba70***\n* *** \ud55c\uae00\ub85c \uc124\uba85\ub41c \ubd80\ubd84\uc740 \uc81c\uac00 \uc2a4\ud130\ub514 \ud558\uba74\uc11c \uc774\ud574\ud55c \ub0b4\uc6a9\uc744 \ucd94\uac00\ud55c \uac83\uc785\ub2c8\ub2e4. ***\n---", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Lab 5 Logistic Regression Classifier\n* ***lab 5-1 \ud558\uace0 \ub3d9\uc77c\ud558\uac8c \uc791\ub3d9\ud558\uc9c0\ub9cc, input file \uc744 \ub2e4\ub974\uac8c \uac00\uc838\uac11\ub2c8\ub2e4.***\n* ***File asset \uc73c\ub85c insert to code \ud558\uba74  pandas code \ub85c \uc0dd\uc131\ub418\uae30 \ub54c\ubb38\uc5d0,  numpy \ub85c \uc804\ud658\ud558\ub294 \uc791\uc5c5\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.*** ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf\nimport numpy as np\ntf.set_random_seed(777)  # for reproducibility"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 3, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>-0.294118</th>\n      <th>0.487437</th>\n      <th>0.180328</th>\n      <th>-0.292929</th>\n      <th>0</th>\n      <th>0.00149028</th>\n      <th>-0.53117</th>\n      <th>-0.0333333</th>\n      <th>0.1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.882353</td>\n      <td>-0.145729</td>\n      <td>0.081967</td>\n      <td>-0.414141</td>\n      <td>0.000000</td>\n      <td>-0.207153</td>\n      <td>-0.766866</td>\n      <td>-0.666667</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058824</td>\n      <td>0.839196</td>\n      <td>0.049180</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.305514</td>\n      <td>-0.492741</td>\n      <td>-0.633333</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.882353</td>\n      <td>-0.105528</td>\n      <td>0.081967</td>\n      <td>-0.535354</td>\n      <td>-0.777778</td>\n      <td>-0.162444</td>\n      <td>-0.923997</td>\n      <td>0.000000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.376884</td>\n      <td>-0.344262</td>\n      <td>-0.292929</td>\n      <td>-0.602837</td>\n      <td>0.284650</td>\n      <td>0.887276</td>\n      <td>-0.600000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.411765</td>\n      <td>0.165829</td>\n      <td>0.213115</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.236960</td>\n      <td>-0.894962</td>\n      <td>-0.700000</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   -0.294118  0.487437  0.180328  -0.292929         0  0.00149028  -0.53117  \\\n0  -0.882353 -0.145729  0.081967  -0.414141  0.000000   -0.207153 -0.766866   \n1  -0.058824  0.839196  0.049180   0.000000  0.000000   -0.305514 -0.492741   \n2  -0.882353 -0.105528  0.081967  -0.535354 -0.777778   -0.162444 -0.923997   \n3   0.000000  0.376884 -0.344262  -0.292929 -0.602837    0.284650  0.887276   \n4  -0.411765  0.165829  0.213115   0.000000  0.000000   -0.236960 -0.894962   \n\n   -0.0333333  0.1  \n0   -0.666667    1  \n1   -0.633333    0  \n2    0.000000    1  \n3   -0.600000    0  \n4   -0.700000    1  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "\nimport sys\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share your notebook.\nclient_ace6f71a1b9946cbb684ca0d9e6c34c0 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='xBNCWwO4mooXZRHuqZ71nRmyIlQt8ca3ZWN8pOo56X69Dx',   ## masking, use your own\n    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_ace6f71a1b9946cbb684ca0d9e6c34c0.get_object(Bucket='tensorflowlabwithwatsonstudio-donotdelete-pr-neiwaip4a29fcg',Key='data-03-diabetes.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_data_1 = pd.read_csv(body)\ndf_data_1.head()\n\n"
        }, 
        {
            "source": "* ***pandas \ub85c \uc77d\uc5c8\uae30 \ub54c\ubb38\uc5d0  numpy\ub85c convert \ud574\uc918\uc57c \ud569\ub2c8\ub2e4.***\n\n[pd to numpy reference link](https://stackoverflow.com/questions/40972147/convert-pandas-pd-to-numpy-array-and-back)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "(758, 8) (758, 1)\n"
                }
            ], 
            "source": "xy = df_data_1.values\n#xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\nprint(x_data.shape, y_data.shape)"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# placeholders for a tensor that will be always fed.\nX = tf.placeholder(tf.float32, shape=[None, 8])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\nW = tf.Variable(tf.random_normal([8, 1]), name='weight')\nb = tf.Variable(tf.random_normal([1]), name='bias')\n\n# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\nhypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n\n# cost/loss function\ncost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\ntrain = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n\n# Accuracy computation\n# True if hypothesis>0.5 else False\npredicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0 0.828086\n500 0.715032\n1000 0.669607\n1500 0.634058\n2000 0.606034\n2500 0.583908\n3000 0.566361\n3500 0.552346\n4000 0.541053\n4500 0.531868\n5000 0.524322\n5500 0.518065\n6000 0.51283\n6500 0.508413\n7000 0.504656\n7500 0.501439\n8000 0.498665\n8500 0.496258\n9000 0.494157\n9500 0.492313\n10000 0.490687\n"
                }
            ], 
            "source": "# Launch graph\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nfor step in range(10001):\n    cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n    if step % 500 == 0:\n        print(step, cost_val)"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\nHypothesis:  [[ 0.91558039]\n [ 0.22683522]\n [ 0.93606353]\n [ 0.33854681]\n [ 0.70947021]\n [ 0.94387567]\n [ 0.63418144]\n [ 0.26217601]\n [ 0.46605211]\n [ 0.647578  ]\n [ 0.20207596]\n [ 0.26094097]\n [ 0.35577044]\n [ 0.74865079]\n [ 0.48198602]\n [ 0.70029336]\n [ 0.91255689]\n [ 0.81196183]\n [ 0.5600341 ]\n [ 0.64852798]\n [ 0.1085592 ]\n [ 0.62158   ]\n [ 0.6815443 ]\n [ 0.38928407]\n [ 0.92176944]\n [ 0.51300806]\n [ 0.57887459]\n [ 0.72995311]\n [ 0.43030149]\n [ 0.94746941]\n [ 0.78300023]\n [ 0.57396239]\n [ 0.82060015]\n [ 0.37257335]\n [ 0.65679973]\n [ 0.83231843]\n [ 0.58754331]\n [ 0.4831382 ]\n [ 0.37859002]\n [ 0.77286875]\n [ 0.15560597]\n [ 0.4124487 ]\n [ 0.09964141]\n [ 0.60118902]\n [ 0.92161381]\n [ 0.75076461]\n [ 0.73070532]\n [ 0.90559077]\n [ 0.93368894]\n [ 0.91864413]\n [ 0.22811721]\n [ 0.40429467]\n [ 0.96732706]\n [ 0.24933477]\n [ 0.4874171 ]\n [ 0.165512  ]\n [ 0.74516839]\n [ 0.87808496]\n [ 0.50680178]\n [ 0.94101411]\n [ 0.70664757]\n [ 0.67294621]\n [ 0.834916  ]\n [ 0.57730496]\n [ 0.57168728]\n [ 0.94220465]\n [ 0.62689906]\n [ 0.86114347]\n [ 0.66229117]\n [ 0.28780806]\n [ 0.68786025]\n [ 0.90430069]\n [ 0.9306736 ]\n [ 0.86206836]\n [ 0.79977643]\n [ 0.5012421 ]\n [ 0.85210168]\n [ 0.90799403]\n [ 0.91445047]\n [ 0.84808767]\n [ 0.81338573]\n [ 0.33132738]\n [ 0.7994501 ]\n [ 0.55860454]\n [ 0.87573439]\n [ 0.46707672]\n [ 0.88494897]\n [ 0.92993057]\n [ 0.75388789]\n [ 0.82213253]\n [ 0.62639564]\n [ 0.6754638 ]\n [ 0.59310734]\n [ 0.90283489]\n [ 0.97351283]\n [ 0.90020257]\n [ 0.5896396 ]\n [ 0.28937638]\n [ 0.62896389]\n [ 0.50749284]\n [ 0.94670266]\n [ 0.79781181]\n [ 0.78918761]\n [ 0.74153799]\n [ 0.70816922]\n [ 0.92441618]\n [ 0.79606128]\n [ 0.48339853]\n [ 0.43081889]\n [ 0.91035944]\n [ 0.87460464]\n [ 0.49460968]\n [ 0.39236531]\n [ 0.62137628]\n [ 0.86665136]\n [ 0.87126905]\n [ 0.90246171]\n [ 0.19139904]\n [ 0.74109066]\n [ 0.83531022]\n [ 0.58707011]\n [ 0.60854357]\n [ 0.90352362]\n [ 0.73347777]\n [ 0.84272301]\n [ 0.77885479]\n [ 0.5850482 ]\n [ 0.5175668 ]\n [ 0.48965061]\n [ 0.46008912]\n [ 0.79509306]\n [ 0.91796875]\n [ 0.8439157 ]\n [ 0.79546094]\n [ 0.85532111]\n [ 0.43280682]\n [ 0.80182236]\n [ 0.67386341]\n [ 0.72966069]\n [ 0.89222407]\n [ 0.65206522]\n [ 0.59239841]\n [ 0.72459126]\n [ 0.89452493]\n [ 0.76167762]\n [ 0.45622423]\n [ 0.91985011]\n [ 0.61778498]\n [ 0.75127083]\n [ 0.27434099]\n [ 0.39669302]\n [ 0.14736211]\n [ 0.27754053]\n [ 0.92134291]\n [ 0.87677658]\n [ 0.92655927]\n [ 0.15666281]\n [ 0.45647946]\n [ 0.77319545]\n [ 0.63008779]\n [ 0.875377  ]\n [ 0.38103467]\n [ 0.80035871]\n [ 0.63236856]\n [ 0.63829732]\n [ 0.72460669]\n [ 0.82048744]\n [ 0.72281408]\n [ 0.6450339 ]\n [ 0.8918702 ]\n [ 0.89605653]\n [ 0.94576478]\n [ 0.24086806]\n [ 0.79243147]\n [ 0.27165306]\n [ 0.43013808]\n [ 0.41218606]\n [ 0.83272403]\n [ 0.70738196]\n [ 0.90879393]\n [ 0.89969409]\n [ 0.56061983]\n [ 0.18676265]\n [ 0.23388617]\n [ 0.5214045 ]\n [ 0.70139182]\n [ 0.59215754]\n [ 0.81487197]\n [ 0.61984968]\n [ 0.37699437]\n [ 0.33371592]\n [ 0.90786844]\n [ 0.37626255]\n [ 0.85946846]\n [ 0.88890517]\n [ 0.73305321]\n [ 0.66720849]\n [ 0.63811308]\n [ 0.54929757]\n [ 0.72365153]\n [ 0.92630786]\n [ 0.78473645]\n [ 0.78525835]\n [ 0.16818282]\n [ 0.26319289]\n [ 0.90825593]\n [ 0.21621215]\n [ 0.93253374]\n [ 0.28269118]\n [ 0.23923157]\n [ 0.5457502 ]\n [ 0.68266809]\n [ 0.27332839]\n [ 0.76965833]\n [ 0.71632969]\n [ 0.80756819]\n [ 0.69819713]\n [ 0.22273482]\n [ 0.32510105]\n [ 0.70172822]\n [ 0.57574755]\n [ 0.91101128]\n [ 0.91967088]\n [ 0.68144178]\n [ 0.46275899]\n [ 0.06130559]\n [ 0.66625059]\n [ 0.36382195]\n [ 0.50444198]\n [ 0.92649817]\n [ 0.64587069]\n [ 0.93885517]\n [ 0.27010593]\n [ 0.15910012]\n [ 0.2812129 ]\n [ 0.68908489]\n [ 0.91506082]\n [ 0.87063372]\n [ 0.65609294]\n [ 0.72821951]\n [ 0.58990306]\n [ 0.1656543 ]\n [ 0.56401122]\n [ 0.17171514]\n [ 0.60486269]\n [ 0.82999116]\n [ 0.72050947]\n [ 0.63157117]\n [ 0.92710751]\n [ 0.82208693]\n [ 0.80418044]\n [ 0.78815359]\n [ 0.79092264]\n [ 0.84863985]\n [ 0.45770001]\n [ 0.4337371 ]\n [ 0.54149693]\n [ 0.81469995]\n [ 0.66022193]\n [ 0.69759864]\n [ 0.81768554]\n [ 0.38877138]\n [ 0.56154495]\n [ 0.6633184 ]\n [ 0.62085319]\n [ 0.46972024]\n [ 0.89382046]\n [ 0.7019583 ]\n [ 0.91086799]\n [ 0.58417624]\n [ 0.77779782]\n [ 0.82213241]\n [ 0.81778526]\n [ 0.6500138 ]\n [ 0.86362678]\n [ 0.38477388]\n [ 0.59334826]\n [ 0.64387399]\n [ 0.32271138]\n [ 0.7675184 ]\n [ 0.30278426]\n [ 0.61656499]\n [ 0.9292118 ]\n [ 0.77368557]\n [ 0.83085936]\n [ 0.73224938]\n [ 0.53130287]\n [ 0.73041749]\n [ 0.45657903]\n [ 0.51545584]\n [ 0.62251621]\n [ 0.58255494]\n [ 0.66491115]\n [ 0.6031692 ]\n [ 0.25289634]\n [ 0.7016803 ]\n [ 0.88283765]\n [ 0.49272051]\n [ 0.55358529]\n [ 0.75522476]\n [ 0.47732821]\n [ 0.71682179]\n [ 0.54415691]\n [ 0.72013474]\n [ 0.89200288]\n [ 0.68607497]\n [ 0.66297895]\n [ 0.88386619]\n [ 0.57712793]\n [ 0.85315818]\n [ 0.91227871]\n [ 0.29258767]\n [ 0.79469019]\n [ 0.22380722]\n [ 0.79053253]\n [ 0.79389507]\n [ 0.69737858]\n [ 0.30650789]\n [ 0.79073668]\n [ 0.69866103]\n [ 0.76641303]\n [ 0.20871821]\n [ 0.80000776]\n [ 0.83584285]\n [ 0.58557421]\n [ 0.94417077]\n [ 0.33680514]\n [ 0.63481081]\n [ 0.94016415]\n [ 0.26709184]\n [ 0.53874534]\n [ 0.6643219 ]\n [ 0.34191209]\n [ 0.1887123 ]\n [ 0.8227641 ]\n [ 0.89784646]\n [ 0.85074556]\n [ 0.57418263]\n [ 0.69449753]\n [ 0.55540293]\n [ 0.80731434]\n [ 0.78948408]\n [ 0.92070031]\n [ 0.75003749]\n [ 0.76064503]\n [ 0.52801424]\n [ 0.92541015]\n [ 0.93651706]\n [ 0.75255185]\n [ 0.24352576]\n [ 0.74467027]\n [ 0.46507737]\n [ 0.75843716]\n [ 0.22434875]\n [ 0.28586465]\n [ 0.43714765]\n [ 0.63165116]\n [ 0.41301835]\n [ 0.57883579]\n [ 0.86751008]\n [ 0.61852169]\n [ 0.84802264]\n [ 0.92162311]\n [ 0.66614735]\n [ 0.11347937]\n [ 0.55778867]\n [ 0.86528593]\n [ 0.86618531]\n [ 0.73604167]\n [ 0.31478551]\n [ 0.84137797]\n [ 0.90327382]\n [ 0.35654384]\n [ 0.5810219 ]\n [ 0.82493484]\n [ 0.81836516]\n [ 0.87947911]\n [ 0.90042108]\n [ 0.84335977]\n [ 0.91391677]\n [ 0.6869024 ]\n [ 0.5834074 ]\n [ 0.54740453]\n [ 0.83811384]\n [ 0.87819874]\n [ 0.27568412]\n [ 0.81216007]\n [ 0.8497656 ]\n [ 0.3256636 ]\n [ 0.66676146]\n [ 0.86900538]\n [ 0.5745458 ]\n [ 0.88125074]\n [ 0.29935756]\n [ 0.83045757]\n [ 0.57996041]\n [ 0.86609417]\n [ 0.39208227]\n [ 0.78441864]\n [ 0.70206428]\n [ 0.76522893]\n [ 0.10566785]\n [ 0.28473657]\n [ 0.64228743]\n [ 0.81146675]\n [ 0.48806447]\n [ 0.75819623]\n [ 0.56178272]\n [ 0.346008  ]\n [ 0.84313387]\n [ 0.472525  ]\n [ 0.89001095]\n [ 0.79934251]\n [ 0.65504241]\n [ 0.91070879]\n [ 0.7180267 ]\n [ 0.81915587]\n [ 0.38118619]\n [ 0.28863695]\n [ 0.76759148]\n [ 0.47015557]\n [ 0.4169561 ]\n [ 0.88026327]\n [ 0.87363988]\n [ 0.90177017]\n [ 0.93958652]\n [ 0.63162535]\n [ 0.89045972]\n [ 0.44911507]\n [ 0.37169147]\n [ 0.45512187]\n [ 0.92510593]\n [ 0.56622463]\n [ 0.14566402]\n [ 0.92441648]\n [ 0.81736875]\n [ 0.58585709]\n [ 0.82144177]\n [ 0.04069211]\n [ 0.90346271]\n [ 0.74272078]\n [ 0.76284468]\n [ 0.74653357]\n [ 0.95459306]\n [ 0.61104304]\n [ 0.79816759]\n [ 0.73084301]\n [ 0.88213575]\n [ 0.24779212]\n [ 0.6717099 ]\n [ 0.89764988]\n [ 0.60244697]\n [ 0.71719921]\n [ 0.92835987]\n [ 0.84781927]\n [ 0.85774237]\n [ 0.40984535]\n [ 0.79919255]\n [ 0.94600612]\n [ 0.772497  ]\n [ 0.65984386]\n [ 0.35792044]\n [ 0.48236805]\n [ 0.53656119]\n [ 0.66566139]\n [ 0.46029183]\n [ 0.76315314]\n [ 0.5425961 ]\n [ 0.74858034]\n [ 0.79249793]\n [ 0.69315434]\n [ 0.61218452]\n [ 0.52114952]\n [ 0.54663038]\n [ 0.93309718]\n [ 0.82335633]\n [ 0.35152954]\n [ 0.49596873]\n [ 0.59410846]\n [ 0.16428573]\n [ 0.87044162]\n [ 0.1466839 ]\n [ 0.90506238]\n [ 0.84390634]\n [ 0.83053392]\n [ 0.69930375]\n [ 0.88902587]\n [ 0.35235035]\n [ 0.74715763]\n [ 0.92941189]\n [ 0.30390531]\n [ 0.43959138]\n [ 0.82018387]\n [ 0.87381953]\n [ 0.70499861]\n [ 0.83159596]\n [ 0.80639005]\n [ 0.77493703]\n [ 0.25823647]\n [ 0.78575701]\n [ 0.9209708 ]\n [ 0.58638167]\n [ 0.79026639]\n [ 0.71035647]\n [ 0.79298306]\n [ 0.85396564]\n [ 0.92902529]\n [ 0.60405469]\n [ 0.38435584]\n [ 0.79762489]\n [ 0.69292831]\n [ 0.95384228]\n [ 0.72983122]\n [ 0.71405441]\n [ 0.4476065 ]\n [ 0.72850901]\n [ 0.92176092]\n [ 0.93667603]\n [ 0.86563933]\n [ 0.70348567]\n [ 0.64793926]\n [ 0.81957704]\n [ 0.5251295 ]\n [ 0.83218169]\n [ 0.80350345]\n [ 0.90912825]\n [ 0.63374549]\n [ 0.63487142]\n [ 0.89414334]\n [ 0.48982587]\n [ 0.48122007]\n [ 0.68749535]\n [ 0.72335136]\n [ 0.66265261]\n [ 0.87879097]\n [ 0.90356308]\n [ 0.19249482]\n [ 0.19523311]\n [ 0.76276231]\n [ 0.48502129]\n [ 0.16623019]\n [ 0.82979971]\n [ 0.89668095]\n [ 0.63640749]\n [ 0.93097395]\n [ 0.92178708]\n [ 0.73179287]\n [ 0.84267646]\n [ 0.66757601]\n [ 0.64559901]\n [ 0.74222225]\n [ 0.60267377]\n [ 0.16993302]\n [ 0.90112036]\n [ 0.88129926]\n [ 0.68537724]\n [ 0.92115009]\n [ 0.86986512]\n [ 0.88673097]\n [ 0.57988113]\n [ 0.71032834]\n [ 0.87540454]\n [ 0.60779256]\n [ 0.85589659]\n [ 0.91526431]\n [ 0.54102635]\n [ 0.84173852]\n [ 0.8599841 ]\n [ 0.58241618]\n [ 0.51495546]\n [ 0.10615861]\n [ 0.27284649]\n [ 0.82862735]\n [ 0.6343075 ]\n [ 0.68368495]\n [ 0.57348222]\n [ 0.93420762]\n [ 0.47606531]\n [ 0.78231025]\n [ 0.28016615]\n [ 0.87591821]\n [ 0.40627661]\n [ 0.75276881]\n [ 0.55791146]\n [ 0.88607484]\n [ 0.59017533]\n [ 0.23924109]\n [ 0.81267965]\n [ 0.9619137 ]\n [ 0.39043644]\n [ 0.93185186]\n [ 0.82267046]\n [ 0.83124113]\n [ 0.77607578]\n [ 0.44469243]\n [ 0.34069005]\n [ 0.75117683]\n [ 0.21298352]\n [ 0.93447727]\n [ 0.35751855]\n [ 0.91378605]\n [ 0.89183253]\n [ 0.49195626]\n [ 0.21114947]\n [ 0.68446428]\n [ 0.48103347]\n [ 0.79434651]\n [ 0.59714669]\n [ 0.97188574]\n [ 0.54971588]\n [ 0.65150958]\n [ 0.74469525]\n [ 0.78634536]\n [ 0.0839409 ]\n [ 0.79442894]\n [ 0.80780756]\n [ 0.80130249]\n [ 0.61498708]\n [ 0.46204844]\n [ 0.58108497]\n [ 0.89838868]\n [ 0.638744  ]\n [ 0.74711442]\n [ 0.80255789]\n [ 0.81288528]\n [ 0.78871465]\n [ 0.57219571]\n [ 0.76387072]\n [ 0.88440716]\n [ 0.72531068]\n [ 0.93444145]\n [ 0.75422472]\n [ 0.6179902 ]\n [ 0.45535409]\n [ 0.82808715]\n [ 0.82645833]\n [ 0.50662327]\n [ 0.61399037]\n [ 0.33095682]\n [ 0.50840384]\n [ 0.81258756]\n [ 0.94313872]\n [ 0.84903771]\n [ 0.7070896 ]\n [ 0.76437402]\n [ 0.8760649 ]\n [ 0.60681605]\n [ 0.91421908]\n [ 0.58300638]\n [ 0.82763469]\n [ 0.28003755]\n [ 0.11872131]\n [ 0.22048157]\n [ 0.36023885]\n [ 0.7363742 ]\n [ 0.78858835]\n [ 0.59807575]\n [ 0.7337898 ]\n [ 0.83839434]\n [ 0.48104936]\n [ 0.42838922]\n [ 0.91141391]\n [ 0.83905041]\n [ 0.43539032]\n [ 0.67418146]\n [ 0.19553916]\n [ 0.33112043]\n [ 0.76801628]\n [ 0.75290751]\n [ 0.89895177]\n [ 0.97273773]\n [ 0.26108316]\n [ 0.77065873]\n [ 0.57348573]\n [ 0.44831258]\n [ 0.71469629]\n [ 0.69655108]\n [ 0.90729976]\n [ 0.68282497]\n [ 0.51930141]\n [ 0.59764379]\n [ 0.12835494]\n [ 0.70590317]\n [ 0.56935245]\n [ 0.8918885 ]\n [ 0.51912212]\n [ 0.55077523]\n [ 0.77543378]\n [ 0.69306934]\n [ 0.54073524]\n [ 0.74501592]\n [ 0.62745309]\n [ 0.33317944]\n [ 0.66335297]\n [ 0.85598677]\n [ 0.83229625]\n [ 0.62406653]\n [ 0.83749425]\n [ 0.28915554]\n [ 0.86110377]\n [ 0.62749517]\n [ 0.72351879]\n [ 0.51437384]\n [ 0.69258094]\n [ 0.80262488]\n [ 0.25715128]\n [ 0.3026323 ]\n [ 0.79870832]\n [ 0.82600707]\n [ 0.80147529]\n [ 0.8791188 ]\n [ 0.81200254]\n [ 0.69092143]\n [ 0.7089653 ]\n [ 0.70113212]\n [ 0.70274872]\n [ 0.77814227]\n [ 0.46352133]\n [ 0.33879516]\n [ 0.88908708]\n [ 0.75822347]\n [ 0.56201899]\n [ 0.29372454]\n [ 0.88285887]\n [ 0.77598298]\n [ 0.85535687]\n [ 0.61462766]\n [ 0.88655883]\n [ 0.89320368]\n [ 0.77866429]\n [ 0.46591926]\n [ 0.91460812]\n [ 0.90927577]\n [ 0.2929917 ]\n [ 0.18105097]\n [ 0.68512964]\n [ 0.40934622]\n [ 0.82737494]\n [ 0.37037545]\n [ 0.45442641]\n [ 0.4463945 ]\n [ 0.75564814]\n [ 0.86274487]\n [ 0.15382649]\n [ 0.38818759]\n [ 0.57733315]\n [ 0.44793954]\n [ 0.54667723]\n [ 0.77243644]\n [ 0.15433373]\n [ 0.91584551]\n [ 0.25025842]\n [ 0.82721698]\n [ 0.7107082 ]\n [ 0.74678147]\n [ 0.79912752]\n [ 0.73135597]\n [ 0.88305366]] \nCorrect (Y):  [[ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]\n [ 1.]] \nAccuracy:  0.762533\n"
                }
            ], 
            "source": "# Accuracy report\nh, c, a = sess.run([hypothesis, predicted, accuracy],\n                  feed_dict={X: x_data, Y: y_data})\nprint(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
        }, 
        {
            "source": "* ***accuracy \uac00 \ub192\uc9c0\ub294 \uc54a\ub124\uc694***     \n'''\n0 0.82794\n200 0.755181\n400 0.726355\n600 0.705179\n800 0.686631\n...\n9600 0.492056\n9800 0.491396\n10000 0.490767\n\n...\n\n [ 1.]\n [ 1.]\n [ 1.]]\nAccuracy:  0.762846\n'''", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}