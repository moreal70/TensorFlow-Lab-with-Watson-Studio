{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "---\n* *** \uc6d0\ubcf8 \uc18c\uc2a4\ub294 \ud64d\ucf69\uacfc\uae30\ub300 \uae40\uc131\ud6c8 \uad50\uc218\ub2d8\uc758  [Github link](https://github.com/hunkim/DeepLearningZeroToAll)\ub97c \ucc38\uc870\ud558\uc138\uc694 ***     \n* *** Watson studio notebook \uc5d0\uc11c \uc791\uc5c5 \uac00\ub2a5\ud558\ub3c4\ub85d \uc218\uc815\ud558\uc600\uc73c\uba70***\n* *** \ud55c\uae00\ub85c \uc124\uba85\ub41c \ubd80\ubd84\uc740 \uc81c\uac00 \uc2a4\ud130\ub514 \ud558\uba74\uc11c \uc774\ud574\ud55c \ub0b4\uc6a9\uc744 \ucd94\uac00\ud55c \uac83\uc785\ub2c8\ub2e4. ***\n---", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "C:\\Users\\HyeonSuPARK\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "WARNING:tensorflow:From C:\\Users\\HyeonSuPARK\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse the retry module or similar alternatives.\n"
                }
            ], 
            "source": "# Lab 10 MNIST and High-level TF API\nfrom tensorflow.contrib.layers import fully_connected, batch_norm, dropout\nfrom tensorflow.contrib.framework import arg_scope\nimport tensorflow as tf\nimport random\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\n\n# parameters\nlearning_rate = 0.01  # we can use large learning rate using Batch Normalization\ntraining_epochs = 15\nbatch_size = 100\nkeep_prob = 0.7\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\ntrain_mode = tf.placeholder(tf.bool, name='train_mode')\n\n# layer output size\nhidden_output_size = 512\nfinal_output_size = 10"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "xavier_init = tf.contrib.layers.xavier_initializer()\nbn_params = {\n    'is_training': train_mode,\n    'decay': 0.9,\n    'updates_collections': None\n}"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "WARNING:tensorflow:From <ipython-input-3-87c91111162a>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee tf.nn.softmax_cross_entropy_with_logits_v2.\n\n"
                }
            ], 
            "source": "# We can build short code using 'arg_scope' to avoid duplicate code\n# same function with different arguments\nwith arg_scope([fully_connected],\n               activation_fn=tf.nn.relu,\n               weights_initializer=xavier_init,\n               biases_initializer=None,\n               normalizer_fn=batch_norm,\n               normalizer_params=bn_params\n               ):\n    hidden_layer1 = fully_connected(X, hidden_output_size, scope=\"h1\")\n    h1_drop = dropout(hidden_layer1, keep_prob, is_training=train_mode)\n    hidden_layer2 = fully_connected(h1_drop, hidden_output_size, scope=\"h2\")\n    h2_drop = dropout(hidden_layer2, keep_prob, is_training=train_mode)\n    hidden_layer3 = fully_connected(h2_drop, hidden_output_size, scope=\"h3\")\n    h3_drop = dropout(hidden_layer3, keep_prob, is_training=train_mode)\n    hidden_layer4 = fully_connected(h3_drop, hidden_output_size, scope=\"h4\")\n    h4_drop = dropout(hidden_layer4, keep_prob, is_training=train_mode)\n    hypothesis = fully_connected(h4_drop, final_output_size, activation_fn=None, scope=\"hypothesis\")\n\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())"
        }, 
        {
            "source": "", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "WARNING:tensorflow:From <ipython-input-4-c73d4a8925b7>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nWARNING:tensorflow:From C:\\Users\\HyeonSuPARK\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\nWARNING:tensorflow:From C:\\Users\\HyeonSuPARK\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nWARNING:tensorflow:From C:\\Users\\HyeonSuPARK\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nWARNING:tensorflow:From C:\\Users\\HyeonSuPARK\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From C:\\Users\\HyeonSuPARK\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n[Epoch:    1] cost = 0.391331553\n[Epoch:    2] cost = 0.332146545\n[Epoch:    3] cost = 0.320681618\n[Epoch:    4] cost = 0.31531054\n[Epoch:    5] cost = 0.312679769\n[Epoch:    6] cost = 0.309612217\n[Epoch:    7] cost = 0.308129908\n[Epoch:    8] cost = 0.306536459\n[Epoch:    9] cost = 0.304321032\n[Epoch:   10] cost = 0.304645187\n[Epoch:   11] cost = 0.303989255\n[Epoch:   12] cost = 0.301637655\n[Epoch:   13] cost = 0.303553812\n[Epoch:   14] cost = 0.301913038\n[Epoch:   15] cost = 0.301574513\nLearning Finished!\n"
                }
            ], 
            "source": "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict_train = {X: batch_xs, Y: batch_ys, train_mode: True}\n        feed_dict_cost = {X: batch_xs, Y: batch_ys, train_mode: False}\n        opt = sess.run(optimizer, feed_dict=feed_dict_train)\n        c = sess.run(cost, feed_dict=feed_dict_cost)\n        avg_cost += c / total_batch\n\n    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost))\n    #print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n\nprint('Learning Finished!')"
        }, 
        {
            "source": "* ***\uc55e\uc5d0 lab \uc5d0\uc11c softmax \ub294 90% , NN\uc740 94%, xavier \ucd08\uae30\ud654\ub97c \uc4f0\uba74 97%, layer 5\uac1c\ub97c \uc4f0\uba74 98.11%, dropout \uc740 98.24% \uc815\ub3c4\uc758 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4.***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Test model and check accuracy\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint('Accuracy:', sess.run(accuracy, feed_dict={\n      X: mnist.test.images, Y: mnist.test.labels, train_mode: False}))"
        }, 
        {
            "source": "* ***sample \uc744 random \ud558\uac8c \uac00\uc838\uc640\uc11c \uadf8\ub9bc\uc73c\ub85c \ubcf4\uc5ec\uc8fc\uace0 \uc608\uce21\ud569\ub2c8\ub2e4. ***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Get one and predict\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(\"Prediction: \", sess.run(\n    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], train_mode: False}))\n\nplt.imshow(mnist.test.images[r:r + 1].\n          reshape(28, 28), cmap='Greys', interpolation='nearest')\nplt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "'''\n[Epoch:    1] cost = 0.519417209\n[Epoch:    2] cost = 0.432551052\n[Epoch:    3] cost = 0.404978843\n[Epoch:    4] cost = 0.392039919\n[Epoch:    5] cost = 0.382165317\n[Epoch:    6] cost = 0.377987834\n[Epoch:    7] cost = 0.372577601\n[Epoch:    8] cost = 0.367208552\n[Epoch:    9] cost = 0.365525589\n[Epoch:   10] cost = 0.361964276\n[Epoch:   11] cost = 0.359540287\n[Epoch:   12] cost = 0.356423751\n[Epoch:   13] cost = 0.354478216\n[Epoch:   14] cost = 0.353212552\n[Epoch:   15] cost = 0.35230893\nLearning Finished!\nAccuracy: 0.9826\n'''"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}