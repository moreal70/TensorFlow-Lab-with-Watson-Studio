{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "---\n* *** \uc6d0\ubcf8 \uc18c\uc2a4\ub294 \ud64d\ucf69\uacfc\uae30\ub300 \uae40\uc131\ud6c8 \uad50\uc218\ub2d8\uc758  [Github link](https://github.com/hunkim/DeepLearningZeroToAll)\ub97c \ucc38\uc870\ud558\uc138\uc694 ***     \n* *** Watson studio notebook \uc5d0\uc11c \uc791\uc5c5 \uac00\ub2a5\ud558\ub3c4\ub85d \uc218\uc815\ud558\uc600\uc73c\uba70***\n* *** \ud55c\uae00\ub85c \uc124\uba85\ub41c \ubd80\ubd84\uc740 \uc81c\uac00 \uc2a4\ud130\ub514 \ud558\uba74\uc11c \uc774\ud574\ud55c \ub0b4\uc6a9\uc744 \ucd94\uac00\ud55c \uac83\uc785\ub2c8\ub2e4. ***\n---", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Lab 4 Multi-variable linear regression with Data asset\n* ***input \uc744 \ud654\uc77c\uc5d0\uc11c \ubc1b\uc2b5\ub2c8\ub2e4.***\n* ***numpy \uc77d\uc5b4\ub4e4\uc774\uace0, x, y \ub370\uc774\ud130\ub97c \ubd84\ub9ac\ud569\ub2c8\ub2e4.  \ub098\uc911\uc5d0 learning step \uc5d0\uc11c input \uc73c\ub85c \ub123\uc2b5\ub2c8\ub2e4.***\n* ***insert to code \ud558\uba74 pandas dataframe \ubc16\uc5d0 \uc5c6\uc2b5\ub2c8\ub2e4. ***\n* ***\uc5ec\uae30\uc11c \uae30\uc874 \uc18c\uc2a4\ub294 numpy \ub97c \uc774\uc6a9\ud558\ubbc0\ub85c, panda to numpy \ubcc0\ud658\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf\nimport numpy as np\ntf.set_random_seed(777)  # for reproducibility\n\n## xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)\n## \uc544\ub798 source \ub85c \ub300\uccb4\ud569\ub2c8\ub2e4. by  \"insert to code\""
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\nimport sys\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share your notebook.\nclient_ace6f71a1b9946cbb684ca0d9e6c34c0 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='XXBNCWwO4mooXZRHuqZ71nRmyIlQt8ca3ZWN8pOo56X69DX',\n    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_ace6f71a1b9946cbb684ca0d9e6c34c0.get_object(Bucket='tensorflowlabwithwatsonstudio-donotdelete-pr-neiwaip4a29fcg',Key='data-01-test-score.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf = pd.read_csv(body,header=None)\n\n### pandas to numpy\nxy = df.values\n\n"
        }, 
        {
            "source": "* ***\uc798\ub77c\ub0b4\uae30 \uc2e0\uacf5, \ubcc4\uac74 \uc544\ub2c8\uc9c0\ub9cc ^^***\n* ***\ub2e8, \uc5ec\uae30\uc11c row \uc218\ub97c \uc815\ud655\ud558\uac8c \ubaa8\ub974\ub2c8, \uc77c\ub2e8\uc740 ' : ' \uc804\uccb4, \ub4a4\uc5d0 shape \uc5d0\uc11c  None \uc73c\ub85c \uc9c0\uc815\ud558\ub294 \uac81\ub2c8\ub2e4.***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "(25, 3) [[ 73  80  75]\n [ 93  88  93]\n [ 89  91  90]\n [ 96  98 100]\n [ 73  66  70]\n [ 53  46  55]\n [ 69  74  77]\n [ 47  56  60]\n [ 87  79  90]\n [ 79  70  88]\n [ 69  70  73]\n [ 70  65  74]\n [ 93  95  91]\n [ 79  80  73]\n [ 70  73  78]\n [ 93  89  96]\n [ 78  75  68]\n [ 81  90  93]\n [ 88  92  86]\n [ 78  83  77]\n [ 82  86  90]\n [ 86  82  89]\n [ 78  83  85]\n [ 76  83  71]\n [ 96  93  95]] 25\n(25, 1) [[152]\n [185]\n [180]\n [196]\n [142]\n [101]\n [149]\n [115]\n [175]\n [164]\n [141]\n [141]\n [184]\n [152]\n [148]\n [192]\n [147]\n [183]\n [177]\n [159]\n [177]\n [175]\n [175]\n [149]\n [192]]\n"
                }
            ], 
            "source": "x_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\n# Make sure the shape and data are OK\nprint(x_data.shape, x_data, len(x_data))\nprint(y_data.shape, y_data)"
        }, 
        {
            "source": "* ***shape \uc120\uc5b8\uc5d0\uc11c None \uc73c\ub85c matrix \uc9c0\uc815\ud558\ub294 \uac83\ub9cc \ub2e4\ub974\uace0 \ub098\uba38\uc9c0\ub294 \ub3d9\uc77c\ud569\ub2c8\ub2e4.***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# placeholders for a tensor that will be always fed.\nX = tf.placeholder(tf.float32, shape=[None, 3])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\nW = tf.Variable(tf.random_normal([3, 1]), name='weight')\nb = tf.Variable(tf.random_normal([1]), name='bias')\n\n# Hypothesis\nhypothesis = tf.matmul(X, W) + b\n\n# Simplified cost/loss function\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\n# Minimize\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\ntrain = optimizer.minimize(cost)\n\n# Launch the graph in a session.\nsess = tf.Session()\n# Initializes global variables in the graph.\nsess.run(tf.global_variables_initializer())"
        }, 
        {
            "source": "* ***Learning ***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0 Cost:  8013.9 \nPrediction:\n [[ 74.1805954 ]\n [ 80.63321686]\n [ 83.98022461]\n [ 91.3227005 ]\n [ 59.21071243]\n [ 41.89090729]\n [ 70.11859131]\n [ 54.8239975 ]\n [ 72.92267609]\n [ 66.10010529]\n [ 65.14825439]\n [ 60.24366379]\n [ 86.90501404]\n [ 71.78623199]\n [ 69.10598755]\n [ 82.41497803]\n [ 65.88251495]\n [ 86.28957367]\n [ 84.18405914]\n [ 76.09278107]\n [ 81.27962494]\n [ 75.88126373]\n [ 78.24948883]\n [ 75.09364319]\n [ 85.10977936]]\n100 Cost:  24.4822 \nPrediction:\n [[ 158.00280762]\n [ 181.74298096]\n [ 183.41183472]\n [ 199.68835449]\n [ 136.35003662]\n [  98.92388916]\n [ 151.2467041 ]\n [ 114.97437286]\n [ 167.59532166]\n [ 153.99279785]\n [ 143.34011841]\n [ 137.5322876 ]\n [ 189.55923462]\n [ 157.03633118]\n [ 150.66976929]\n [ 185.05744934]\n [ 147.09121704]\n [ 183.6157074 ]\n [ 181.98199463]\n [ 163.57327271]\n [ 176.43994141]\n [ 170.78768921]\n [ 168.92460632]\n [ 159.43598938]\n [ 189.80511475]]\n200 Cost:  22.976 \nPrediction:\n [[ 157.79986572]\n [ 181.84764099]\n [ 183.32929993]\n [ 199.67205811]\n [ 136.45574951]\n [  99.15608978]\n [ 151.24610901]\n [ 114.97987366]\n [ 167.86390686]\n [ 154.43540955]\n [ 143.3678894 ]\n [ 137.74169922]\n [ 189.40486145]\n [ 156.82632446]\n [ 150.72940063]\n [ 185.2015686 ]\n [ 146.91007996]\n [ 183.58068848]\n [ 181.76945496]\n [ 163.35870361]\n [ 176.46313477]\n [ 170.93536377]\n [ 168.89619446]\n [ 159.06864929]\n [ 189.82427979]]\n300 Cost:  21.5944 \nPrediction:\n [[ 157.60520935]\n [ 181.94837952]\n [ 183.25032043]\n [ 199.65628052]\n [ 136.55780029]\n [  99.37872314]\n [ 151.24475098]\n [ 114.9835968 ]\n [ 168.12145996]\n [ 154.85906982]\n [ 143.39425659]\n [ 137.942276  ]\n [ 189.25740051]\n [ 156.62599182]\n [ 150.78578186]\n [ 185.33973694]\n [ 146.7379303 ]\n [ 183.54582214]\n [ 181.56616211]\n [ 163.15332031]\n [ 176.48460388]\n [ 171.07691956]\n [ 168.86830139]\n [ 158.71723938]\n [ 189.84315491]]\n400 Cost:  20.3268 \nPrediction:\n [[ 157.41848755]\n [ 182.04536438]\n [ 183.17477417]\n [ 199.64097595]\n [ 136.65635681]\n [  99.59223938]\n [ 151.24266052]\n [ 114.98565674]\n [ 168.36845398]\n [ 155.26460266]\n [ 143.41926575]\n [ 138.13444519]\n [ 189.11653137]\n [ 156.43486023]\n [ 150.83911133]\n [ 185.47227478]\n [ 146.57437134]\n [ 183.51113892]\n [ 181.37173462]\n [ 162.95671082]\n [ 176.50450134]\n [ 171.21264648]\n [ 168.84094238]\n [ 158.38105774]\n [ 189.86174011]]\n500 Cost:  19.1639 \nPrediction:\n [[ 157.23939514]\n [ 182.13871765]\n [ 183.10249329]\n [ 199.62615967]\n [ 136.75154114]\n [  99.79699707]\n [ 151.23994446]\n [ 114.9861908 ]\n [ 168.60533142]\n [ 155.65278625]\n [ 143.44303894]\n [ 138.31852722]\n [ 188.98200989]\n [ 156.25256348]\n [ 150.88954163]\n [ 185.59934998]\n [ 146.41900635]\n [ 183.47666931]\n [ 181.18580627]\n [ 162.76850891]\n [ 176.52287292]\n [ 171.3427887 ]\n [ 168.81410217]\n [ 158.0594635 ]\n [ 189.88002014]]\n600 Cost:  18.0968 \nPrediction:\n [[ 157.06761169]\n [ 182.22862244]\n [ 183.03337097]\n [ 199.61180115]\n [ 136.84347534]\n [  99.99336243]\n [ 151.23658752]\n [ 114.98529816]\n [ 168.83251953]\n [ 156.02436829]\n [ 143.46559143]\n [ 138.49488831]\n [ 188.85353088]\n [ 156.07868958]\n [ 150.93716431]\n [ 185.72122192]\n [ 146.27145386]\n [ 183.44241333]\n [ 181.00799561]\n [ 162.58837891]\n [ 176.53985596]\n [ 171.46755981]\n [ 168.78778076]\n [ 157.75183105]\n [ 189.89805603]]\n700 Cost:  17.1176 \nPrediction:\n [[ 156.90281677]\n [ 182.31520081]\n [ 182.96722412]\n [ 199.59788513]\n [ 136.93226624]\n [ 100.18170166]\n [ 151.23265076]\n [ 114.98306274]\n [ 169.0504303 ]\n [ 156.38006592]\n [ 143.48698425]\n [ 138.66386414]\n [ 188.7308197 ]\n [ 155.91285706]\n [ 150.98217773]\n [ 185.83811951]\n [ 146.1313324 ]\n [ 183.40843201]\n [ 180.8379364 ]\n [ 162.41593933]\n [ 176.55545044]\n [ 171.5872345 ]\n [ 168.76197815]\n [ 157.45756531]\n [ 189.91578674]]\n800 Cost:  16.2189 \nPrediction:\n [[ 156.74472046]\n [ 182.39855957]\n [ 182.90394592]\n [ 199.5843811 ]\n [ 137.01803589]\n [ 100.36233521]\n [ 151.22821045]\n [ 114.97958374]\n [ 169.25942993]\n [ 156.72055054]\n [ 143.5072937 ]\n [ 138.82572937]\n [ 188.6136322 ]\n [ 155.75469971]\n [ 151.02467346]\n [ 185.95024109]\n [ 145.99830627]\n [ 183.37472534]\n [ 180.67530823]\n [ 162.25088501]\n [ 176.56976318]\n [ 171.70196533]\n [ 168.73669434]\n [ 157.17604065]\n [ 189.9332428 ]]\n900 Cost:  15.3941 \nPrediction:\n [[ 156.5930481 ]\n [ 182.4788208 ]\n [ 182.84339905]\n [ 199.57128906]\n [ 137.10090637]\n [ 100.53559113]\n [ 151.2232666 ]\n [ 114.97494507]\n [ 169.4598999 ]\n [ 157.04647827]\n [ 143.52653503]\n [ 138.98080444]\n [ 188.50170898]\n [ 155.60385132]\n [ 151.0647583 ]\n [ 186.05776978]\n [ 145.87202454]\n [ 183.34129333]\n [ 180.51977539]\n [ 162.09284973]\n [ 176.58288574]\n [ 171.81199646]\n [ 168.71185303]\n [ 156.90673828]\n [ 189.95039368]]\n1000 Cost:  14.6368 \nPrediction:\n [[ 156.44754028]\n [ 182.55615234]\n [ 182.78549194]\n [ 199.55862427]\n [ 137.18096924]\n [ 100.70178986]\n [ 151.21786499]\n [ 114.96923065]\n [ 169.65220642]\n [ 157.35848999]\n [ 143.5447998 ]\n [ 139.12942505]\n [ 188.39483643]\n [ 155.46005249]\n [ 151.10258484]\n [ 186.16091919]\n [ 145.75219727]\n [ 183.30818176]\n [ 180.37103271]\n [ 161.94158936]\n [ 176.59484863]\n [ 171.91752625]\n [ 168.68756104]\n [ 156.64912415]\n [ 189.96728516]]\n1100 Cost:  13.9417 \nPrediction:\n [[ 156.30789185]\n [ 182.63064575]\n [ 182.73007202]\n [ 199.54632568]\n [ 137.2583313 ]\n [ 100.86119843]\n [ 151.21206665]\n [ 114.96251678]\n [ 169.83666992]\n [ 157.65716553]\n [ 143.56210327]\n [ 139.27180481]\n [ 188.29277039]\n [ 155.32290649]\n [ 151.13825989]\n [ 186.25985718]\n [ 145.63851929]\n [ 183.27539062]\n [ 180.22879028]\n [ 161.79678345]\n [ 176.60575867]\n [ 172.01872253]\n [ 168.66372681]\n [ 156.40267944]\n [ 189.9838562 ]]\n1200 Cost:  13.3034 \nPrediction:\n [[ 156.17393494]\n [ 182.70240784]\n [ 182.67707825]\n [ 199.53440857]\n [ 137.33309937]\n [ 101.01415253]\n [ 151.20585632]\n [ 114.95490265]\n [ 170.01365662]\n [ 157.94309998]\n [ 143.57850647]\n [ 139.40823364]\n [ 188.19532776]\n [ 155.19218445]\n [ 151.171875  ]\n [ 186.35479736]\n [ 145.53067017]\n [ 183.24293518]\n [ 180.09275818]\n [ 161.65817261]\n [ 176.61564636]\n [ 172.11579895]\n [ 168.6403656 ]\n [ 156.16694641]\n [ 190.00018311]]\n1300 Cost:  12.7172 \nPrediction:\n [[ 156.04534912]\n [ 182.77151489]\n [ 182.62637329]\n [ 199.52282715]\n [ 137.40534973]\n [ 101.16085052]\n [ 151.19932556]\n [ 114.94641113]\n [ 170.18345642]\n [ 158.21682739]\n [ 143.59402466]\n [ 139.53897095]\n [ 188.1022644 ]\n [ 155.06752014]\n [ 151.20353699]\n [ 186.4458313 ]\n [ 145.42837524]\n [ 183.21081543]\n [ 179.96264648]\n [ 161.52545166]\n [ 176.6245575 ]\n [ 172.20889282]\n [ 168.61746216]\n [ 155.94140625]\n [ 190.01620483]]\n1400 Cost:  12.1788 \nPrediction:\n [[ 155.92199707]\n [ 182.83811951]\n [ 182.57785034]\n [ 199.51161194]\n [ 137.47520447]\n [ 101.30160522]\n [ 151.19245911]\n [ 114.93713379]\n [ 170.34635925]\n [ 158.47889709]\n [ 143.60874939]\n [ 139.66423035]\n [ 188.01341248]\n [ 154.94871521]\n [ 151.23335266]\n [ 186.53320312]\n [ 145.3314209 ]\n [ 183.17907715]\n [ 179.83824158]\n [ 161.39842224]\n [ 176.63258362]\n [ 172.29818726]\n [ 168.59506226]\n [ 155.72566223]\n [ 190.03196716]]\n1500 Cost:  11.6843 \nPrediction:\n [[ 155.80358887]\n [ 182.90228271]\n [ 182.53143311]\n [ 199.50073242]\n [ 137.54270935]\n [ 101.43666077]\n [ 151.18531799]\n [ 114.92713928]\n [ 170.50267029]\n [ 158.72976685]\n [ 143.62271118]\n [ 139.78427124]\n [ 187.92858887]\n [ 154.83544922]\n [ 151.26138306]\n [ 186.61703491]\n [ 145.23950195]\n [ 183.14768982]\n [ 179.7192688 ]\n [ 161.27680969]\n [ 176.63975525]\n [ 172.3838501 ]\n [ 168.5730896 ]\n [ 155.51928711]\n [ 190.04743958]]\n1600 Cost:  11.23 \nPrediction:\n [[ 155.68997192]\n [ 182.96414185]\n [ 182.48704529]\n [ 199.4901886 ]\n [ 137.60797119]\n [ 101.56624603]\n [ 151.17791748]\n [ 114.91648102]\n [ 170.65264893]\n [ 158.96995544]\n [ 143.63589478]\n [ 139.89932251]\n [ 187.84759521]\n [ 154.72750854]\n [ 151.2877655 ]\n [ 186.69744873]\n [ 145.15240479]\n [ 183.11669922]\n [ 179.60548401]\n [ 161.16036987]\n [ 176.64613342]\n [ 172.46601868]\n [ 168.55157471]\n [ 155.32185364]\n [ 190.06265259]]\n1700 Cost:  10.8125 \nPrediction:\n [[ 155.5809021 ]\n [ 183.02372742]\n [ 182.44454956]\n [ 199.47993469]\n [ 137.67105103]\n [ 101.69056702]\n [ 151.17025757]\n [ 114.90520477]\n [ 170.79656982]\n [ 159.19989014]\n [ 143.64836121]\n [ 140.009552  ]\n [ 187.77023315]\n [ 154.62460327]\n [ 151.31254578]\n [ 186.77462769]\n [ 145.06987   ]\n [ 183.08607483]\n [ 179.49667358]\n [ 161.0488739 ]\n [ 176.6517334 ]\n [ 172.54483032]\n [ 168.53048706]\n [ 155.13296509]\n [ 190.07754517]]\n1800 Cost:  10.4288 \nPrediction:\n [[ 155.47622681]\n [ 183.08119202]\n [ 182.40391541]\n [ 199.47001648]\n [ 137.73208618]\n [ 101.80986786]\n [ 151.16239929]\n [ 114.89337921]\n [ 170.93467712]\n [ 159.4200592 ]\n [ 143.66017151]\n [ 140.11521912]\n [ 187.69642639]\n [ 154.52656555]\n [ 151.33583069]\n [ 186.84867859]\n [ 144.99171448]\n [ 183.05586243]\n [ 179.39259338]\n [ 160.94215393]\n [ 176.65663147]\n [ 172.62046814]\n [ 168.50987244]\n [ 154.95228577]\n [ 190.09220886]]\n1900 Cost:  10.0762 \nPrediction:\n [[ 155.37574768]\n [ 183.13658142]\n [ 182.36503601]\n [ 199.46037292]\n [ 137.79109192]\n [ 101.92436981]\n [ 151.15437317]\n [ 114.88105011]\n [ 171.06724548]\n [ 159.63084412]\n [ 143.6713562 ]\n [ 140.21647644]\n [ 187.62594604]\n [ 154.43312073]\n [ 151.35766602]\n [ 186.91973877]\n [ 144.91772461]\n [ 183.02604675]\n [ 179.29310608]\n [ 160.83996582]\n [ 176.66087341]\n [ 172.69302368]\n [ 168.48963928]\n [ 154.7794342 ]\n [ 190.10661316]]\n2000 Cost:  9.75209 \nPrediction:\n [[ 155.27926636]\n [ 183.18994141]\n [ 182.32781982]\n [ 199.45101929]\n [ 137.84814453]\n [ 102.03425598]\n [ 151.14614868]\n [ 114.86825562]\n [ 171.19445801]\n [ 159.83267212]\n [ 143.68191528]\n [ 140.3135376 ]\n [ 187.55865479]\n [ 154.34410095]\n [ 151.37814331]\n [ 186.98794556]\n [ 144.84768677]\n [ 182.99661255]\n [ 179.19792175]\n [ 160.74212646]\n [ 176.66447449]\n [ 172.76263428]\n [ 168.46984863]\n [ 154.61407471]\n [ 190.12072754]]\n"
                }
            ], 
            "source": "for step in range(2001):\n    cost_val, hy_val, _ = sess.run(\n        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n    if step % 100 == 0:\n        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
        }, 
        {
            "source": "* ***\uc0c8\ub85c\uc6b4 input \uac12\uc744 \ub123\uace0 \uc608\uce21\ud574\ubcf4\uae30 ***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Your score will be  [[ 175.88304138]]\nOther scores will be  [[ 177.93075562]\n [ 180.93258667]]\n"
                }
            ], 
            "source": "# Ask my score\nprint(\"Your score will be \", sess.run(\n    hypothesis, feed_dict={X: [[100, 70, 101]]}))\n\nprint(\"Other scores will be \", sess.run(hypothesis,\n                                        feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}