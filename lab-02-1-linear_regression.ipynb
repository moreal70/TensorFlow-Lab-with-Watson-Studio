{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* *** 원본 소스는 홍콩과기대 김성훈 교수님의  [Github link](https://github.com/hunkim/DeepLearningZeroToAll)를 참조하세요 ***     \n",
    "* *** Watson studio notebook 에서 작업 가능하도록 수정하고 제가 study 하면서 이해한 내용을 첨언하였습니다.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-02-1 linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reprducibilty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " * ***x 는 분석 대상이 되는 값으로 feature 라고 하고, y 는 결과 예측 값입니다. 여기서는 지도 학습 (learning) 을 위한 정답이라고 생각하면 됩니다.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***W는 1차함수의 기울기고,b 는 y 절편입니다.***\n",
    "* ***우리의 목표는 결국 최적의 W & b를 찾아내는 것입니다.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try to find value for W and b to compute y_data = x_data * W + b  \n",
    "# We know that W should be 1 and b should be 0\n",
    "# But let's TensorFlow figure it out \n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***W & b 를 가지고 1차 함수를 만들었습니다. ***\n",
    "* ***hypothesis는 중학교때 배운 y 값에 해당하는 정답이라고 주어진 y_train 값하고의 차를 제곱해서 평균을 낸 것을, cost 또는 loss 라고 얘기합니다.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Out hypothesis XW+b\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***중요한 것은 여기서 이 cost 값을 최소화 (즉 0에 가까운) 하는 W,b 를 찾아내는 것이 목표 입니다.***\n",
    "* ***이를 위한 사용하는 GradientDescent 모듈은 learning rate 만큼 움직여 가면서 minimum(cost) 에 도달할때까지 반복적으로 돌립니다. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***사실 앞 부분은 선언만 하는 것이고요***\n",
    "* ***이제부터, 실제로 실행을 합니다. 선행 lab 에서 설명했듯이  session 정의하고 run 해줘야 합니다. (요 사이에,, init 한번 수행)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***2000번을 돌립니다.  중간에 snap shot 데이터를 찍어보면,, cost 값이 점점 0 으로 수렴한다는 것을 알 수 있습니다. ***\n",
    "* ***우리의 목표인 최적의 W & b를 찾아낸 것입니다.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.82329 [ 2.12867713] [-0.85235667]\n",
      "200 0.0699668 [ 1.30721486] [-0.69837117]\n",
      "400 0.0267167 [ 1.18983996] [-0.4315508]\n",
      "600 0.0102017 [ 1.11730957] [-0.26667204]\n",
      "800 0.00389553 [ 1.07249022] [-0.16478711]\n",
      "1000 0.0014875 [ 1.04479456] [-0.10182849]\n",
      "1200 0.000567998 [ 1.02768016] [-0.06292368]\n",
      "1400 0.000216891 [ 1.01710474] [-0.03888312]\n",
      "1600 8.28196e-05 [ 1.01056981] [-0.02402747]\n",
      "1800 3.16242e-05 [ 1.00653136] [-0.01484741]\n",
      "2000 1.20761e-05 [ 1.00403607] [-0.00917497]\n"
     ]
    }
   ],
   "source": [
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 200 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***상식적으로 아래 좌표를 그림으로 그려본다면,,, 기울기 1이고 , y 절편이 0인 1차원 그래프를 그릴수 있습니다.***\n",
    "* ***직관적으로 알아낼수 있겠으나, 주어진 값이 복잡해지면 불가능할 것이고,  이를 찾아내기 위해서 Linear regression 을 수행한것입니다. ***     \n",
    "x_train = [1, 2, 3]     \n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "### 왜 W,b 를 찾은 것이지요 ?   그 것은  예측입니다.  새로운 x 값이 나오면 y 값을 예측할 수 있는 것이지요. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
